# Neural Network 1: XOR 문제 해결 방법, Backpropagation  

### XOR 문제 딥러닝으로 풀기

앞선 자료에서 설명한 Neural Network를 이용해 XOR을 해결했던 방법에 대해 알아보자. 앞서 XOR로 인해 ANN(Artificial Neural Network) 분야의 암흑기가 찾아왔다고 설명했다. 당시 XOR이 문제가 되었던 이유는 당시 ANN의 형태인 선형 구조로는 XOR 구조의 문제를 해결할 수 없었기 때문이었다.  

이를 해결결하기 위해서는 다음과 같은 가정이 필요하다. 2진수 입력 x1과 x2를 갖고, 출력 S(y)를 가지는 Neural이 두 개 존재한다고 할 때(여기서의 S()는 시그모이드 함수) 이 두 Neural의 출력 S(y1)과 S(y2)를 입력으로 갖는 출력 y\`가 존재한다고 말이다. 즉, x1과 x2의 XOR을 y\`이라고 하고 이 경우 다음과 같은 모양을 지닌다(이때 사용되는 Neural의 가중치 W와 바이어스 b는 사용자가 지정한 수여야 한다는 점에 유의하자).  

![](/assets/2nd_week_1.png)  

더불어 위와 같이 두 개의 Neural로 구성된 Network를 하나의 Neural로 합쳐 간략화할 수 있다. 그 방법은 다음의 모양을 지닌다.  

![](/assets/2nd_week_2.png)  

이처럼 X에는 x1과 x2의 경우의 수를 넣고, W에는 가중치들의 2차원 행렬, b는 y1과 y2의 바이어스가 들어가게 된다. 이를 수식으로 나타내면 다음과 같이 표현할 수 있다.  

```
K(X) = sigmoid(XW1 + B1)  
y` = sigmoid(K(X)W2 + B2)  
```

![](/assets/2nd_week_3.png)  

이를 TensorFlow를 이용한 Python에서 표현하면 다음과 같이 표현할 수 있다.

```
K = tf.sigmoid(tf.matmul(X, W1) + b1)  
hypothesis = tf.sigmoid(tf.matmul(K, W2) + b2)  
```

### 미분 정리

Backpropagation을 하기 위해서는 간단한 미분이 필요하다. 미분을 수학적으로 표현하면 아래의 그림에서 **d/dx*\f(x)**로 표현할 수 있다. 또한, f를 x로 미분한다고 이야기할 수 있다.

![](/assets/2nd_week_4.png)

위의 식에서 우항을 설명하자면 델타x를 아주 작은 값(0에 가까운 값)으로 보낼 때 함수 f에 x와 델타 x를 더한 것과 델타 x를 더하지 않은 것의 차이를 아주 작은 값 델타 x로 나누어 **순간 변화율(기울기)'**을 구하는 방법이다.  

> f는 얼마나 변하니?  
 
그래서 이 미분은 Gradient descent 알고리즘에서 필수적인 것이다. 위의 그림의 문제를 풀면서 이해하도록 하자.    

1번 문제의 경우는 상수 함수를 미분하는 것이고, 이는 0이 나온다.  

2번 문제는 x라는 변수로 함수를 미분하는 것으로, 이는 1을 나타낸다.  

3번 문제는 2x로 함수를 미분하는 것으로, x는 미분되어 사라지고 2만 남아 결국 2를 나타낸다.  

다음 문제도 풀어보자.  

![](/assets/2nd_week_5.png)  

![](/assets/2nd_week_6.png)  

[여기][this]를 참고하여 위의 문제가 어떻게 결과에 도달하는지 확인해보자. 추가적으로 까다로운 문제가 존재한다.이는 *Chain Rule*이 적용되는데, f(g(x))의 형태를 띄고 있다. 이 경우 g(x)를 미분한 값과 f(g)를 미분한 값의 곱으로 문제를 해결할 수 있다. [여기][this]의 영상을 참고하여 보다 쉽게 이해하도록 하자.  

[this]: https://www.youtube.com/watch?v=oZyvmtqLmLo&feature=youtu.be

### Deep Network 학습시키기  

위에서 언급한 미분의 방법을 기계적으로 어떻게 학습시킬 것인가에 대해 살펴보자. 앞서 도출된 y`\를 가지고 cost함수를 정의할 수 있는데, 이 cost 함수는 2차 방정식의 그래프(U형)를 띈다. 즉, 기울기를 구하면서 그래프의 최저점으로 도달하는 모양을 띄는 것이다. 이 최저점은 Global Minimum이라고 하며, 이 Global Minimum을 구하는 과정을 Gradient descent Algorithm이라고 한다.  

여기서 주목해야 할 점은 기울기는 미분 값이므로 cost함수는 미분 값을 이용하여 초기의 W값을 조정해가며 Global Minimum에 도달하는 방법이라는 것이다. 당시의 학자들은 이 방법을 이용해 ANN을 구현하게 되면 아주 복잡해지고, layer가 많아지면 불가능하다고 할 정도였다.

시간이 흘러 이 문제점을 해결하고자 Hinton 교수는 Backpropagation을 고안해낸다. Backpropagation은 최종 값 f가 도출되기 까지의 과정을 뒤로 되감는(forward) 방식이다. 그렇게 뒤로 되감으면서 최초의 입력에 도달해 최초의 입력 x가 f에 미치는 영향을 구해 x의 W 값을 바꿔가며 f를 Global Minimum에 도달시켜 결과를 도출하는 방법이다. 여기서 사용되는 것이 바로 미분 값이다.  

![](/assets/2nd_week_7.png)  

위에 그림에서 처럼 f가 -7일 때 f에 대한 b의 미분 값을 구해 b가 f에 끼치는 영향을 구할 수 있는 것이다. 부가적으로 f와 b를 미분한 값이 1이라는 의미는 b와 f가 1:1로 변경된다는 의미이다. b가 +1이 되어 2가 되면, f도 +1이 되어 -6이 된다는 의미라고 보면 된다.  

이와 마찬가지로 node의 수가 아무리 많아져도 최초의 입력 x가 f에 미치는 영향(f와 x의 미분 값)을 구할 수 있고, 아무리 복잡한 식도 미분 값을 구할 수 있다면 Backpropagation할 수 있다는 것이다.  

그렇다면 Tensorflow에서는 이를 어떻게 구현하고 있을까?  

![](/assets/2nd_week_8.png)  

이와 같이 연산자나 함수를 node로 만들어 하나의 그래프로 표현하고 있다. 여기서 hypothesis(가설)이라는 변수에 sigmoid 값을 넣고, 이 hypothesis를 이용하여 cost 함수의 미분 값을 구할 수 있다.  

![](/assets/2nd_week_9.png)  

물론 tensorflow를 사용한다면, 이 모든 동작은 TensorBoard library로 정의되어 있다. 사용자는 사용하기만 하면 되는 것이다. 그러니 구현해야 한다는 우려를 하지 않아도 괜찮다.  

지금까지 설명한 방법을 이용하면 아무리 복잡해도 Backpropagation을 이용하여 기울기(미분 값)을 구할 수 있다는 것이다. 이를 이해했다면, 69년에 불가능하다고 하던 이론을 해결한 것이니 자랑스럽게 생각해도 좋다.  

